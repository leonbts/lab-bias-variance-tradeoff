{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be5d83a-b30e-4ecd-b1a7-b7761a0fbc1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, Lasso\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab528f10-f47b-4acc-a620-a139d0c2f51f",
   "metadata": {},
   "source": [
    "# Challenge 1 - clean the data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b40e303-b97b-4da6-9ac0-6c097bfcc055",
   "metadata": {},
   "source": [
    "Import the data in ../data/economies_of_scale.csv into a pandas dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b36b8b0-b6fa-4db8-80a2-d40e32e9fe95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here\n",
    "\n",
    "profit = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4728b180-935b-4e6b-a860-ac24c1a23969",
   "metadata": {},
   "source": [
    "Notice the column *total_profit* is in text. You first need to clean this data to turn it back into numeric data. You can consider using the method *apply* or *replace* to do this"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1d959e-75e4-4974-b974-613b63ed2189",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean the column total_profit to make it numeric\n",
    "\n",
    "profit['total_profit'] = "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51228b4c-860a-441f-83e3-149002b31521",
   "metadata": {},
   "source": [
    "Now let's visualize how total profitability changes with number of machines sold "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd2c4522-ef4d-43f1-b96e-99eaf1b4d6a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(profit['items_produced'], profit['total_profit'], label=\"Data\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f9f6477-dc69-4487-8392-d5e977fd1537",
   "metadata": {},
   "source": [
    "This is an expected phenomenon called [*economies of scale*](https://en.wikipedia.org/wiki/Economies_of_scale): If you sell a very niche product (few sales) you can usuallyy charge a premium and have good profitability, since there will likely not be sufficient market for many competitors. As your product becomes more mainstream and more players enter the market, the supply pressures lower your profitability. Finally, once you break through to become a large player, as you expand your operations, your fixed costs become more and more diluted by the total number of items sold and your profitability increases again due to your lower operating costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87b6b72-a78a-43cf-aed8-aa7e6615dde7",
   "metadata": {},
   "source": [
    "In this exercise we are going to try to predict the profitability of the company based on the number of items sold.\n",
    "Select your Features (X) and your Targets (y) and perform a test-train split."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcad258-d1c0-4117-947e-0395e1ba415b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "#X =\n",
    "#y =\n",
    "#..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59e4d672-60e8-4b3b-bbd0-dce2d01be622",
   "metadata": {},
   "source": [
    "# Degree 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec1882e7-663a-4e0f-9178-94c92dee3c4d",
   "metadata": {},
   "source": [
    "Let's first approach this problem with a simple linear regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "378189fe-b1c7-48a4-b3cf-b14ab861d4eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_poly_train = scaler.fit_transform(X_train)\n",
    "X_poly_test = scaler.transform(X_test)\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly_train, y_train)\n",
    "\n",
    "y_train_pred = model.predict(X_poly_train)\n",
    "y_test_pred = model.predict(X_poly_test)\n",
    "train_error = mean_squared_error(y_train, y_train_pred)\n",
    "test_error = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "plt.scatter(X, y, label=\"Data\")\n",
    "X_plot = pd.DataFrame(np.linspace(100, 2200, 100).reshape(-1, 1), columns=['items_produced'])\n",
    "y_plot = model.predict(scaler.transform(X_plot))\n",
    "plt.plot(X_plot, y_plot, label=\"Degree 1\", linewidth=2)\n",
    "plt.xlim(X['items_produced'].min(), X['items_produced'].max())\n",
    "plt.ylim(y.min(), y.max())\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c7aea0-e93b-4042-8a51-162872ab7d0f",
   "metadata": {},
   "source": [
    "We can easily see this is not a good model. It completely misses the niche side of the market and it does not approximate well  the profitability of the large players. This is a weak model with little generalization power.\n",
    "This model exhibits *high bias*: the data DOES NOT fit well with the training data. It is however a nodel with *low variance*: the profit predicted  swings in expected ways with the sold units.\n",
    "This is typical in cases of *Underfitting*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb3c469-37b1-4f49-817d-49b2bc00ef3b",
   "metadata": {},
   "source": [
    "# Degree 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38b58c09-0dc0-44b7-82dc-d66e60f29ab6",
   "metadata": {},
   "source": [
    "We are going to introdduce a common regression technique called *polynomial regression*. When you have a single feature $X$ and you apply a linear regression you are finding the optimal coefficientf for the formula\n",
    "\n",
    "$$ y = \\beta_0 + \\beta_1 X $$\n",
    "\n",
    "We are going to create new features by considering powers of the base feature $X$, so a *quadratic* regression would produce the optimal coefficients for the formula\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 X + \\beta_2 X^2$$\n",
    "\n",
    "and a *cubic* regression would produce the optimal coefficients for the formula\n",
    "\n",
    "$$y = \\beta_0 + \\beta_1 X + \\beta_2 X^2 + \\beta_3 X^3$$\n",
    "\n",
    "and so on.\n",
    "\n",
    "This way, with a single feature *X* you can produce multiple features $X^2$, $X^3$, etc to apply a higher dimensional Linear Regression. We create these extra features $X^2$, $X^3$ using a sklearn preprocessing tool called *PolynomialFeatures*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e63030-7508-4ea7-b23e-c6a07d200ade",
   "metadata": {},
   "outputs": [],
   "source": [
    "#polynomial features of degree 1 gives us a constant term plus the linear term \n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "X_example = [[1],[2],[3],[5]]\n",
    "poly_features = PolynomialFeatures(degree=1)\n",
    "poly_features.fit_transform(X_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "329992ae-b27d-424e-90c5-065d8493871c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#polynomial features of degree 2 gives us a constant term plus the linear term, plus a quadratic term \n",
    "X_example = [[1],[2],[3],[5]]\n",
    "poly_features = PolynomialFeatures(degree=2)\n",
    "poly_features.fit_transform(X_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baf0eb1-c301-44e3-94af-c49707182439",
   "metadata": {},
   "outputs": [],
   "source": [
    "#polynomial features of degree 2 gives us a constant term plus the linear term, plus a cubic term \n",
    "X_example = [[1],[2],[3],[5]]\n",
    "poly_features = PolynomialFeatures(degree=3)\n",
    "poly_features.fit_transform(X_example)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a7bf7d-836f-48b4-afa1-47844f91d267",
   "metadata": {},
   "source": [
    "And so on. So, if we want to use a quadratic approximation to this problem we run the following code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e25c291-a2e4-4d42-85c5-0469ce1043a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures, StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "poly_features = PolynomialFeatures(degree=2)  # here is where the 2 comes in\n",
    "X_poly_train = scaler.fit_transform(poly_features.fit_transform(X_train))\n",
    "X_poly_test = scaler.transform(poly_features.transform(X_test))\n",
    "\n",
    "model = LinearRegression()\n",
    "model.fit(X_poly_train, y_train)\n",
    "\n",
    "y_train_pred = model.predict(X_poly_train)\n",
    "y_test_pred = model.predict(X_poly_test)\n",
    "train_error = mean_squared_error(y_train, y_train_pred)\n",
    "test_error = mean_squared_error(y_test, y_test_pred)\n",
    "\n",
    "plt.scatter(X, y, label=\"Data\")\n",
    "X_plot = pd.DataFrame(np.linspace(100, 2200, 100).reshape(-1, 1), columns=['items_produced'])\n",
    "y_plot = model.predict(scaler.transform(poly_features.transform(X_plot)))\n",
    "plt.plot(X_plot, y_plot, label=\"Degree 2\", linewidth=2)\n",
    "plt.xlim(X['items_produced'].min(), X['items_produced'].max())\n",
    "plt.ylim(y.min(), y.max())\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e888872-3c65-4206-9fbf-b02c2fb99cd3",
   "metadata": {},
   "source": [
    "You can see the fit obviously looks much better just by visual inspection. Let's keep it going..."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcde61a6-cfb5-4367-b3e5-30ad32e9bd2f",
   "metadata": {},
   "source": [
    "# Challenge 2 -Degree 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f1c281-6b06-48e8-8161-653c14a4e608",
   "metadata": {},
   "source": [
    "Replicate the exercise above but now for degree 4. Is the fit improving?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49de84cc-f982-4826-a72a-9550160c2bcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15805cd8-63d2-4626-9543-21e76b5718af",
   "metadata": {},
   "source": [
    "It seems like we keep increasing the fit. The model has more variance, but it also loses more bias in a way that seems like a good tradeoff!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f43c1b91-0d2b-4986-903d-3d65c40f3de3",
   "metadata": {},
   "source": [
    "# Challenge 3 -High degrees"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13089097-3fd2-48de-9c9f-8f7abc0eceaa",
   "metadata": {},
   "source": [
    "so... should we keep this going forever?\n",
    "Replicate the exercise above but now for degrees 12, 16, 20.... Is the fit improving? Why?\n",
    "\n",
    "(Bonus question for you to consider: why am I only asking for even degrees?)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6677ae81-3335-4558-b5ff-9ce41f56328a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f8f685-e1cd-4a24-a2a6-0d7b47e243f6",
   "metadata": {},
   "source": [
    "It looks like the more powerufl models are starting to behave in erratic ways, trying to fit too closely to the training data.\n",
    "They are getting *lower bias*: the data DOES fit better with the training data...\n",
    "... but the cost of that is *higher variance*: the models becomes swingy and it seems that it's not reflecting real patterns in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c871a7fd-8834-4518-b7e9-c15e85e0c4b4",
   "metadata": {},
   "source": [
    "# Challenge 4 -Visualizing errors "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14402ffb-a514-4fdd-8853-7f8da8a169bb",
   "metadata": {},
   "source": [
    "Let's see what happens to the errors.\n",
    "Run the code above in a loop and collect the *train_error* and *test_error* for each degree. Then plot them all."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d327238e-0cb7-4656-a3d3-aa923d836c06",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [1, 2, 4, 12, 16, 20]\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for degree in degrees:\n",
    "    # your code here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4df9b176-1adf-4402-aff3-c813bb22c8a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(degrees, train_errors, label=\"Training Error\")\n",
    "plt.plot(degrees, test_errors, label=\"Test Error\")\n",
    "plt.xlabel(\"Model Complexity (Degree)\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aed7d599-6e04-4b48-a46d-c68b9accccd2",
   "metadata": {},
   "source": [
    "You see that the training error keeps going down. That is expected: higher order models are more powerful but they start having enough power to fit the noise in the training data. Because of this you see the testing error going up at some point: your model believes natural variation in the training data is part of the fundamental mechanist you are trying to learn and will not be able to generalize. It becomes *overfit*."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e0aaedf-c04b-4a12-a714-4d6354d4f384",
   "metadata": {},
   "source": [
    "# Optional Challenge - Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ca996f-ff3b-40f5-8cf0-345c48fbd132",
   "metadata": {},
   "source": [
    "Regularization is a way of controlling overfitting by imposing a price on the variance of the model. \n",
    "Re-run the high dimentionality *degree 20* case above but run a Lasso regression rather than a LinearRegression, with a significant penalty for runaway coefficients.\n",
    "\n",
    "Hint: where you have \n",
    "`model = LinearRegression()`\n",
    "use instead\n",
    "`model = Lasso(alpha=100)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51cd014b-0c77-4714-939d-243fbbe9e5bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "#your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45963a6e-8e90-45f4-83ab-5b3dc7edbf8e",
   "metadata": {},
   "source": [
    "And now redo the test_error vs train_error plotting above, but using a Lasso regession rather than a linear regression.\n",
    "\n",
    "What happens to the bias-variance tradeoff in this case?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00f50075-d007-42c9-8b94-e5468e902c30",
   "metadata": {},
   "outputs": [],
   "source": [
    "degrees = [1, 2, 4, 12, 16, 20]\n",
    "train_errors = []\n",
    "test_errors = []\n",
    "\n",
    "for degree in degrees:\n",
    "    #your code here\n",
    "\n",
    "plt.plot(degrees, train_errors, label=\"Training Error\")\n",
    "plt.plot(degrees, test_errors, label=\"Test Error\")\n",
    "plt.xlabel(\"Model Complexity (Degree)\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
